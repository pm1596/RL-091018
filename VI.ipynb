{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VI.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "qi0ECv_XPFf2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function, division\n",
        "from builtins import range\n",
        "\n",
        "import numpy as np\n",
        "from Grid import standard_grid\n",
        "from Utility import print_values, print_policy\n",
        "\n",
        "\n",
        "Theta = 1e-3\n",
        "Gamma = 0.9\n",
        "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')\n",
        "\n",
        "def best_action_value(grid,V,s):\n",
        "  best_a = None\n",
        "  best_value = float('-inf')\n",
        "  grid.set_state(s)\n",
        "  \n",
        "  for a in ALL_POSSIBLE_ACTIONS:\n",
        "    trainsitions = grid.get_transition_probs(a)\n",
        "    expected_v = 0\n",
        "    expected_r = 0\n",
        "    for(prob,r,state_prime) in transitions:\n",
        "      expected_v += prob * r\n",
        "      expected_r += prob * V[state_prime]\n",
        "    V = expected_r + Gamma * expected_v\n",
        "    if V > best_value:\n",
        "      best_value = V\n",
        "      Best_a = a\n",
        "  return best_a,best_value\n",
        "\n",
        "def calculate_values(grid):\n",
        "  \n",
        "  v = {}\n",
        "  states = grid.all_states()\n",
        "  \n",
        "  for s in states:\n",
        "    v[s] = 0\n",
        "    while True:\n",
        "      biggest_change = 0\n",
        "      for s in grid.non_terminal_states():\n",
        "        old_v = v[s]\n",
        "        _,new_v = best_action_value(grid,V,s)\n",
        "        v[s] = new_v\n",
        "        biggest_change = max(biggest_change,np.abs(old_v - new_v))\n",
        "      \n",
        "      if biggest_change < Theta:\n",
        "        break\n",
        "    return V\n",
        "  \n",
        "def initialize_random_policy():\n",
        "  \n",
        "  policy = {}\n",
        "  \n",
        "  for s in grid.non_terminal_states:\n",
        "    policy[s] = np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
        "  return policy\n",
        "\n",
        "def cal_greedy_policy(grid,V):\n",
        "  policy = initialize_random_policy()\n",
        "  for s in policy.keys():\n",
        "    grid.set_states(s)\n",
        "    best_a,_ = best_action_value(grid,V,s)\n",
        "    policy[s] = best_a\n",
        "  return policy\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  # this grid gives you a reward of -0.1 for every non-terminal state\n",
        "  # we want to see if this will encourage finding a shorter path to the goal\n",
        "  grid = standard_grid(obey_prob=0.8, step_cost=None)\n",
        "\n",
        "  # print rewards\n",
        "  print(\"rewards:\")\n",
        "  print_values(grid.rewards, grid)\n",
        "\n",
        "  # calculate accurate values for each square\n",
        "  V = calculate_values(grid)\n",
        "\n",
        "  # calculate the optimum policy based on our values\n",
        "  policy = calculate_greedy_policy(grid, V)\n",
        "\n",
        "  # our goal here is to verify that we get the same answer as with policy iteration\n",
        "  print(\"values : \")\n",
        "  print_values(V, grid)\n",
        "  print(\"policy : \")\n",
        "  print_policy(policy, grid)\n",
        "\n",
        "  \n",
        "  \n",
        "        \n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
